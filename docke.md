Here's the **updated full script** with the fix to ensure the output directory exists before saving the CSV file. It also includes better error handling to skip repositories if an error occurs and logs skipped repositories in the GitHub Actions output.  

---

### **Updated `jfrog_report.py`**
```python
import os
import requests
import csv
import warnings
from urllib.parse import urljoin
from datetime import datetime
import argparse

# Suppress SSL verification warnings
from requests.packages.urllib3.exceptions import InsecureRequestWarning
warnings.simplefilter('ignore', InsecureRequestWarning)

# Parse command-line arguments
parser = argparse.ArgumentParser(description="Generate JFrog reports for multiple organizations.")
parser.add_argument('--org', required=True, help="The organization name (e.g., 'ev' or 'ppa').")
parser.add_argument('--jfrog-url', required=True, help="The JFrog Artifactory URL.")
parser.add_argument('--output', required=True, help="The output directory for the generated report.")
args = parser.parse_args()

# Get JFrog Artifactory instance details
JFROG_URL = args.jfrog_url
ARTIFACTORY_TOKEN = os.getenv(f'JFROG_API_KEY_{args.org.upper()}')  # Get API key for the given organization

if not ARTIFACTORY_TOKEN:
    print(f"Error: JFROG_API_KEY_{args.org.upper()} is not set in environment variables.")
    exit(1)

# Headers for the API request
headers = {
    'Authorization': f'Bearer {ARTIFACTORY_TOKEN}',
    'Accept': 'application/json',
}

# Function to fetch all repositories
def fetch_repositories():
    """
    Fetches a list of all repositories in the Artifactory instance.
    """
    url = f"{JFROG_URL}/api/repositories"
    response = requests.get(url, headers=headers, verify=False)

    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to fetch repositories: {response.status_code}")
        return []

# Function to list all artifacts in a repository
def list_artifacts(repo_name):
    """
    Recursively fetches all artifacts in the specified repository.
    """
    url = f"{JFROG_URL}/api/storage/{repo_name}?list&deep=1"
    response = requests.get(url, headers=headers, verify=False)

    if response.status_code == 200:
        return response.json().get("files", [])
    else:
        print(f"Failed to list artifacts for {repo_name}: {response.status_code}")
        return []

# Function to process repository artifacts
def process_repository(repo_name, package_type):
    """
    Processes all artifacts in a repository using batch metadata from the recursive listing.
    """
    artifact_list = list_artifacts(repo_name)
    repo_details = []

    for artifact in artifact_list:
        artifact_path = artifact.get("uri", "")
        artifact_url = urljoin(f"{JFROG_URL}/{repo_name}", artifact_path)
        created_date = artifact.get("lastModified", "")

        # Parse package name and version from artifact path
        segments = artifact_path.strip("/").split("/")
        if len(segments) >= 2:
            package_name = f"{repo_name}/{'/'.join(segments[:-2])}"  # Up to the version folder
            version = segments[-2]  # Second-to-last segment
        else:
            package_name = repo_name
            version = ""

        # Append details only for Docker repositories
        if package_type == "Docker":
            repo_details.append({
                "repo_name": repo_name,
                "package_type": package_type,
                "package_name": package_name,
                "version": version,
                "url": artifact_url,
                "created_date": created_date,
                "license": "",  # Leave blank if not available
                "secarch": "",  # Leave blank if not available
                "artifactory_instance": "frigate.jfrog.io"
            })
    
    return repo_details

# Function to save data to CSV
def save_to_csv(data, filename):
    """
    Saves extracted metadata to a CSV file, ensuring the directory exists.
    """
    headers = ['repo_name', 'package_type', 'package_name', 'version', 'url', 'created_date', 'license', 'secarch', 'artifactory_instance']

    # Ensure the output directory exists
    os.makedirs(os.path.dirname(filename), exist_ok=True)

    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=headers)
        writer.writeheader()
        writer.writerows(data)

    print(f"Data has been written to {filename}")

# Main function to process all repositories
def main():
    """
    Main function to process all repositories and extract metadata.
    """
    repositories = fetch_repositories()
    all_repo_details = []
    skipped_repos = []  # List to store skipped repositories

    for repo in repositories:
        repo_name = repo.get("key")
        package_type = repo.get("packageType", "")  # Dynamically fetch the package type
        print(f"Processing repository: {repo_name} (Type: {package_type})")

        try:
            # Process only Docker repositories
            if package_type == "Docker":
                repo_details = process_repository(repo_name, package_type)
                all_repo_details.extend(repo_details)
        except Exception as e:
            print(f"Skipping repository {repo_name} due to error: {e}")
            skipped_repos.append(repo_name)  # Store skipped repo names

    # Save all details to CSV with a dynamic file name
    current_date = datetime.now().strftime('%Y-%m-%d')  # Current date in YYYY-MM-DD format
    output_file = os.path.join(args.output, f"EV_EOL_{current_date}_docker.csv")  # Save only Docker reports
    save_to_csv(all_repo_details, output_file)

    # Log skipped repositories in GitHub Actions output
    if skipped_repos:
        print("\nSkipped repositories due to errors:")
        for repo in skipped_repos:
            print(f"- {repo}")

if __name__ == '__main__':
    main()
```

---

### **Key Fixes & Improvements**
1. **Ensures the Output Directory Exists Before Writing the CSV**  
   - Uses `os.makedirs(os.path.dirname(filename), exist_ok=True)`.
   - Prevents "No such file or directory" errors.

2. **Improved Error Handling**  
   - If an error occurs while processing a repository, it is skipped, and the script continues.
   - Skipped repositories are logged in GitHub Actions.

3. **Logging Skipped Repositories in GitHub Actions Output**  
   - After processing, skipped repositories are listed to improve visibility.

---

### **Next Steps**
1. **Replace the old script with this updated one** in your repository.
2. **Ensure GitHub Secrets are set correctly** (`JFROG_API_KEY_EV`, `JFROG_API_KEY_PPA`).
3. **Trigger the workflow manually or wait for the scheduled run**.

Now, the script should **process all valid repositories**, **skip failing ones**, and **log skipped repositories in GitHub Actions**. ðŸš€
