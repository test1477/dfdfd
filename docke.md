Sure! Hereâ€™s the complete updated script with the necessary fixes, including ensuring the output directory exists before attempting to write the CSV file:

```python
import os
import requests
import csv
import warnings
from urllib.parse import urljoin
from datetime import datetime

# Suppress SSL verification warnings
from requests.packages.urllib3.exceptions import InsecureRequestWarning
warnings.simplefilter('ignore', InsecureRequestWarning)

# JFrog Artifactory instance details
JFROG_URL = 'https://frigate.jfrog.io/artifactory'  # Base URL for JFrog
ARTIFACTORY_TOKEN = 'your_artifactory_api_token'  # Replace with your API token

# Headers for the API request
headers = {
    'Authorization': f'Bearer {ARTIFACTORY_TOKEN}',
    'Accept': 'application/json',
}

# Function to fetch all repositories
def fetch_repositories():
    """
    Fetches a list of all repositories in the Artifactory instance.
    """
    url = f"{JFROG_URL}/api/repositories"
    response = requests.get(url, headers=headers, verify=False)

    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to fetch repositories: {response.status_code}")
        return []

# Function to list all artifacts in a repository
def list_artifacts(repo_name):
    """
    Recursively fetches all artifacts in the specified repository.
    """
    url = f"{JFROG_URL}/api/storage/{repo_name}?list&deep=1"
    response = requests.get(url, headers=headers, verify=False)

    if response.status_code == 200:
        return response.json().get("files", [])
    else:
        print(f"Failed to list artifacts for {repo_name}: {response.status_code}")
        return []

# Function to process repository artifacts without extra API calls
def process_repository(repo_name, package_type):
    """
    Processes all artifacts in a repository using batch metadata from the recursive listing.
    """
    artifact_list = list_artifacts(repo_name)
    repo_details = []

    for artifact in artifact_list:
        artifact_path = artifact.get("uri", "")
        artifact_url = urljoin(f"{JFROG_URL}/{repo_name}", artifact_path)
        created_date = artifact.get("lastModified", "")

        # Parse package name and version from artifact path
        segments = artifact_path.strip("/").split("/")
        if len(segments) >= 2:
            package_name = f"{repo_name}/{'/'.join(segments[:-2])}"  # Up to the version folder
            version = segments[-2]  # Second-to-last segment
        else:
            package_name = repo_name
            version = ""

        # Append details for specific package types like Docker
        if package_type == "Docker" or package_type == "Npm" or package_type == "Helm":
            repo_details.append({
                "repo_name": repo_name,
                "package_type": package_type,
                "package_name": package_name,
                "version": version,
                "url": artifact_url,
                "created_date": created_date,
                "license": "",  # Leave blank if not available
                "secarch": "",  # Leave blank if not available
                "artifactory_instance": "frigate.jfrog.io"
            })
    
    return repo_details

# Function to ensure directory exists before saving the CSV
import os
def save_to_csv(data, filename):
    """
    Saves extracted metadata to a CSV file.
    Ensures the directory exists before saving the file.
    """
    # Ensure the directory exists
    os.makedirs(os.path.dirname(filename), exist_ok=True)

    headers = ['repo_name', 'package_type', 'package_name', 'version', 'url', 'created_date', 'license', 'secarch', 'artifactory_instance']
    
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=headers)
        writer.writeheader()
        writer.writerows(data)
    
    print(f"Data has been written to {filename}")

# Main function to process all repositories
def main():
    """
    Main function to process all repositories and extract metadata.
    """
    repositories = fetch_repositories()
    all_repo_details = []

    for repo in repositories:
        repo_name = repo.get("key")
        package_type = repo.get("packageType", "")  # Dynamically fetch the package type
        print(f"Processing repository: {repo_name} (Type: {package_type})")

        # Process repositories of specific package types
        if package_type in ["Docker", "Npm", "Helm"]:
            try:
                repo_details = process_repository(repo_name, package_type)
                all_repo_details.extend(repo_details)
            except Exception as e:
                print(f"Skipping repository {repo_name} due to error: {e}")

    # Save all details to CSV with a dynamic file name
    current_date = datetime.now().strftime('%Y-%m-%d')  # Current date in YYYY-MM-DD format
    output_file = f"reports/ev/EV_EOL_{current_date}_docker.csv"  # Save only Docker reports
    save_to_csv(all_repo_details, output_file)

if __name__ == '__main__':
    main()
```

### Changes Made:
1. **Directory Creation**: Added `os.makedirs(os.path.dirname(filename), exist_ok=True)` in the `save_to_csv` function to ensure that the output directory (`reports/ev/`) is created if it doesn't exist before trying to write the CSV file.
2. **Package Type Check**: The script now processes repositories of `Docker`, `Npm`, and `Helm` package types, which you might want to adjust based on your needs.
3. **Error Handling**: The `try-except` block ensures the script continues processing repositories even if one fails.

### GitHub Action Workflow

The GitHub Action workflow remains the same:

```yaml
name: Generate JFrog Reports

on:
  schedule:
    - cron: "0 9 * * *"  # Runs daily at 9 AM UTC
  workflow_dispatch:  # Allows manual triggering

jobs:
  generate-report:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set Up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Install Dependencies
        run: pip install requests

      - name: Run Report Script for EV
        env:
          JFROG_API_KEY: ${{ secrets.JFROG_API_KEY_EV }}
        run: |
          python report_script.py --org ev --jfrog-url "https://frigate.io/artifactory" --output reports/ev

      - name: Run Report Script for PPA
        env:
          JFROG_API_KEY: ${{ secrets.JFROG_API_KEY_PPA }}
        run: |
          python report_script.py --org ppa --jfrog-url "https://ppa.jfrog.io/artifactory" --output reports/ppa

      - name: Upload Reports to JFrog (EV)
        run: |
          curl -H "X-JFrog-Art-Api:${{ secrets.JFROG_API_KEY_EV }}" -T reports/ev/EV_EOL_$(date +%F).csv "https://frigate.io/artifactory/reports/ev/EV_EOL_$(date +%F).csv"

      - name: Upload Reports to JFrog (PPA)
        run: |
          curl -H "X-JFrog-Art-Api:${{ secrets.JFROG_API_KEY_PPA }}" -T reports/ppa/EV_EOL_$(date +%F).csv "https://ppa.jfrog.io/artifactory/reports/ppa/EV_EOL_$(date +%F).csv"
```

### Notes:
- The script will now handle repositories of type `Docker`, `Npm`, and `Helm`, but you can adjust it to support other types as necessary.
- Ensure that you have the `JFROG_API_KEY_EV` and `JFROG_API_KEY_PPA` secrets stored in GitHub's repository settings.

Let me know if you need any further adjustments!
